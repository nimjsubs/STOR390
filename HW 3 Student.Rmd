---
title: "HW 3"
author: "Nimalan Subramanian"
date: "2/28/2024"
output: 
  html_document:
    number_sections: true
---

# 

In this homework, we will discuss support vector machines and tree-based methods.  I will begin by simulating some data for you to use with SVM. 

```{r}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y)

```


##

Quite clearly, the above data is not linearly separable.  Create a training-testing partition with 100 random observations in the training partition.  Fit an svm on this training data using the radial kernel, and tuning parameters $\gamma=1$, cost $=1$.  Plot the svm on the training data.  

```{r}
ran <- sample(1:nrow(dat), 100)
train <- dat[ran,]
test <- dat[-ran,]

svmfit <- svm(y ~., data = train, kernel = "radial", gamma = 1, cost = 1)

plot(svmfit, train)
```

##

Notice that the above decision boundary is decidedly non-linear.  It seems to perform reasonably well, but there are indeed some misclassifications.  Let's see if increasing the cost ^[Remember this is a parameter that decides how smooth your decision boundary should be] helps our classification error rate.  Refit the svm with the radial kernel, $\gamma=1$, and a cost of 10000.  Plot this svm on the training data. 

```{r}
svmfit <- svm(y ~., data = train, kernel = "radial", gamma = 1, cost = 10000)

plot(svmfit, train)

```

##

It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model. 

One of the main dangers of increasing the cost as such is potential overfitting. This leads into less generalization with the model, with it being good at predicting the training data but dropping in accuracy with new/unseen data. This in turn makes the model more sensitive to outliers, which can skew decision boundaries to account for the outliers. Furthermore, tuning hyperparameters becomes an issue, making it a challenge to find a balance between the cost and other parameters.

##

Create a confusion matrix by using this svm to predict on the current testing partition.  Comment on the confusion matrix.  Is there any disparity in our classification results?    

```{r}
#remove eval = FALSE in above
table(true=dat[-ran,"y"], pred=predict(svmfit, newdata=dat[-ran,]))
```
There seems to be a disparity in the classification results, which is evident in the prediction of 2s. There are 20 cases of misclassification of 2s, while there are only 18 cases of accurate classification of 2s. This starkly contrasts the accuracy of predicting 1s, which yielded 57 accurate classifications and only 5 cases of misclassification.

##

Is this disparity because of imbalance in the training/testing partition?  Find the proportion of class `2` in your training partition and see if it is broadly representative of the underlying 25\% of class 2 in the data as a whole.  

```{r}
two_prop_train <- sum(train$y == "2")/nrow(train)
two_prop_total <- sum(dat$y == "2")/nrow(dat)

```

The proportion of class 2 in the training set is 26% compared to the 25% of the data as a whole. This explains why class 2 is more likely to be predicted, resulting in the higher rate of class 1 being misclassified as class 2.

##

Let's try and balance the above to solutions via cross-validation.  Using the `tune` function, pass in the training data, and a list of the following cost and $\gamma$ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}.  Save the output of this function in a variable called `tune.out`.  

```{r}

set.seed(1)

cost_vals <- c(0.1, 1, 10, 100, 1000)
gamma_vals <- c(0.5, 1, 2, 3, 4)

tune.out <- tune(svm, train.x = y ~., data = train, kernel = "radial", ranges = list(cost = cost_vals, gamma = gamma_vals))

```

I will take `tune.out` and use the best model according to error rate to test on our data.  I will report a confusion matrix corresponding to the 100 predictions.  


```{r, eval = FALSE}
table(true=dat[-ran,"y"], pred=predict(tune.out$best.model, newdata=dat[-ran,]))
```

##

Comment on the confusion matrix.  How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.  

The classification rate of class 2 has significantly improved, with 21 accurate classifications compared to only 10 cases of misclassification. One of the major improvements comes in a higher accuracy of classification than misclassification. As the data is shown to be non-linear, SVM may not be the best classifier, as it is inherently a linear classifier. While we can improve the rate of classification with SVM, this could lead to issues such as overfitting and reduced generalization. As such, another classifier for non-linear data should be considered for improved results.

# 
Let's turn now to decision trees.  

```{r}

library(kmed)
data(heart)
library(tree)

```

## 

The response variable is currently a categorical variable with four levels.  Convert heart disease into binary categorical variable.  Then, ensure that it is properly stored as a factor. 

```{r}
response = ifelse(heart$class > 0, 1, 0)
ResponseFac = as.factor(response)

heart <- data.frame(heart, ResponseFac)
```

## 

Train a classification tree on a 240 observation training subset (using the seed I have set for you).  Plot the tree.  

```{r}
set.seed(101)

train <- sample(1:nrow(heart), 240)
tree.heart = tree(ResponseFac ~. -class, heart, subset = train)

plot(tree.heart)
text(tree.heart, pretty = 0)
```


## 

Use the trained model to classify the remaining testing points.  Create a confusion matrix to evaluate performance.  Report the classification error rate.  

```{r}
heart.pred = predict(tree.heart, heart[-train,], type="class")
with(heart[-train,], table(heart.pred, ResponseFac))
(3+8)/57
```

##  

Above we have a fully grown (bushy) tree.  Now, cross validate it using the `cv.tree` command.  Specify cross validation to be done according to the misclassification rate.  Choose an ideal number of splits, and plot this tree.  Finally, use this pruned tree to test on the testing set.  Report a confusion matrix and the misclassification rate.  

```{r}
cv.heart <- cv.tree(tree.heart, FUN = prune.misclass)

prune.heart = prune.misclass(tree.heart, best = 19)

heart.pred = predict(prune.heart, heart[-train,], type="class")
with(heart[-train,], table(heart.pred, ResponseFac))
(3+8)/57
```


##

Discuss the trade-off in accuracy and interpretability in pruning the above tree. 

Pruning did not seem to have any impact with the accuracy and interpretability, with the misclassification rate staying the same at around 19.30 %.

## 

Discuss the ways a decision tree could manifest algorithmic bias.  

One factor that can result in algorithmic bias is biased training data. How the data is collected also plays a part in this, with potential sampling bias leading to a potential reduction in generalization. Apart from this, decision trees can be affected by overfitting, playing into bias. This can further result in bias through interpretation, with users applying predictions in a manner that justifies a biased decision.